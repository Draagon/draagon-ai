# Draagon AI Behavior System Design

## Vision

A **dynamic, evolvable behavior system** where:
- Behaviors are pluggable modules that define what an agent CAN DO
- Behaviors can be created, tested, evolved, and enhanced automatically
- A meta-behavior ("Behavior Architect") can create and improve other behaviors
- Behaviors are contextually activated based on user, application, and situation
- Multiple tiers of behaviors: core, add-on, application-provided, auto-generated

---

## Architecture Overview

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         BEHAVIOR SYSTEM                                      │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                      BEHAVIOR REGISTRY                               │   │
│  │  ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌───────────┐ │   │
│  │  │   CORE   │ │  ADD-ON  │ │   APP    │ │   AUTO   │ │   META    │ │   │
│  │  │ Behaviors│ │ Behaviors│ │ Behaviors│ │ Generated│ │ Behaviors │ │   │
│  │  └──────────┘ └──────────┘ └──────────┘ └──────────┘ └───────────┘ │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                    │                                         │
│                                    ▼                                         │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                    BEHAVIOR ACTIVATION ENGINE                        │   │
│  │                                                                      │   │
│  │  Context (user, app, situation) → Active Behavior Set               │   │
│  │                                                                      │   │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐                 │   │
│  │  │ User Perms  │  │ App Config  │  │  Situation  │                 │   │
│  │  │ & Prefs     │  │ & Features  │  │  Detection  │                 │   │
│  │  └─────────────┘  └─────────────┘  └─────────────┘                 │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                    │                                         │
│                                    ▼                                         │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                      BEHAVIOR EXECUTION                              │   │
│  │                                                                      │   │
│  │  Query → Match Triggers → Select Behavior → Execute Actions         │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                    │                                         │
│                                    ▼                                         │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                    BEHAVIOR LIFECYCLE                                │   │
│  │                                                                      │   │
│  │  Create → Test → Deploy → Monitor → Evolve → Retire                 │   │
│  │                                                                      │   │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐                 │   │
│  │  │  Testing    │  │  Metrics    │  │  Evolution  │                 │   │
│  │  │  Framework  │  │  Tracking   │  │  Engine     │                 │   │
│  │  └─────────────┘  └─────────────┘  └─────────────┘                 │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## 1. Behavior Definition

### Core Data Structures

```python
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Callable, Protocol
from datetime import datetime


class BehaviorTier(Enum):
    """Behavior origin and trust level."""
    CORE = "core"              # Built into draagon-ai, heavily tested
    ADDON = "addon"            # Official add-on packages
    APPLICATION = "application" # Provided by host application
    GENERATED = "generated"    # Auto-generated by Behavior Architect
    EXPERIMENTAL = "experimental"  # Under testing, not production-ready


class BehaviorStatus(Enum):
    """Lifecycle status of a behavior."""
    DRAFT = "draft"            # Being developed
    TESTING = "testing"        # Under automated testing
    STAGING = "staging"        # Shadow mode, tracking but not active
    ACTIVE = "active"          # Production ready
    DEPRECATED = "deprecated"  # Being phased out
    RETIRED = "retired"        # No longer available


class ActivationScope(Enum):
    """When this behavior can activate."""
    GLOBAL = "global"          # Always available
    APPLICATION = "application" # Only in specific applications
    USER = "user"              # Only for specific users
    SESSION = "session"        # Only in specific session contexts
    TRIGGER = "trigger"        # Only when specific triggers match


@dataclass
class Action:
    """A single action an agent can take within a behavior."""

    name: str                           # "search_web", "cast_spell"
    description: str                    # Human-readable description

    # Parameters this action accepts
    parameters: dict[str, ActionParameter] = field(default_factory=dict)

    # When this action should be considered
    triggers: list[str] = field(default_factory=list)  # Semantic triggers

    # Examples of this action in use
    examples: list[ActionExample] = field(default_factory=list)

    # Constraints
    requires_confirmation: bool = False
    required_permissions: list[str] = field(default_factory=list)
    cooldown_seconds: int = 0

    # Execution
    handler: str | None = None          # Tool name or handler reference

    # Metadata for evolution
    success_rate: float = 0.0           # Tracked over time
    usage_count: int = 0
    last_used: datetime | None = None


@dataclass
class ActionParameter:
    """Parameter definition for an action."""
    name: str
    description: str
    type: str = "string"                # string, int, float, bool, enum
    required: bool = True
    default: Any = None
    enum_values: list[str] | None = None
    validation_pattern: str | None = None


@dataclass
class ActionExample:
    """Example of action usage for LLM context."""
    user_query: str
    action_call: dict[str, Any]         # {"name": "search_web", "args": {"query": "..."}}
    expected_outcome: str
    is_positive: bool = True            # True = do this, False = don't do this


@dataclass
class Trigger:
    """Defines when a behavior should activate."""

    name: str
    description: str

    # Trigger types (OR logic - any match activates)
    semantic_patterns: list[str] = field(default_factory=list)  # LLM evaluates
    keyword_patterns: list[str] = field(default_factory=list)   # Regex patterns
    intent_categories: list[str] = field(default_factory=list)  # From intent classifier
    context_conditions: list[str] = field(default_factory=list) # e.g., "user.role == 'admin'"

    # Priority for trigger conflicts
    priority: int = 50                   # 0-100, higher wins

    # Conditions that PREVENT activation
    exclusion_patterns: list[str] = field(default_factory=list)


@dataclass
class BehaviorPrompts:
    """Prompt templates for a behavior."""

    # Main decision prompt - decides which action to take
    decision_prompt: str

    # Response synthesis - formats the response
    synthesis_prompt: str

    # Optional specialized prompts
    error_recovery_prompt: str | None = None
    confirmation_prompt: str | None = None
    clarification_prompt: str | None = None

    # Prompt versioning for evolution
    version: str = "1.0.0"
    parent_version: str | None = None   # For tracking evolution


@dataclass
class BehaviorConstraints:
    """Rules and limits for a behavior."""

    # Safety constraints
    requires_user_confirmation: list[str] = field(default_factory=list)  # Actions needing confirm
    blocked_actions: list[str] = field(default_factory=list)  # Never allow these
    rate_limits: dict[str, int] = field(default_factory=dict)  # action -> per_minute

    # Context constraints
    allowed_applications: list[str] | None = None  # None = all
    allowed_users: list[str] | None = None         # None = all
    required_permissions: list[str] = field(default_factory=list)

    # Behavioral constraints (for LLM)
    style_guidelines: list[str] = field(default_factory=list)
    forbidden_topics: list[str] = field(default_factory=list)
    required_disclaimers: list[str] = field(default_factory=list)


@dataclass
class BehaviorMetrics:
    """Tracked metrics for behavior health and evolution."""

    # Usage
    total_activations: int = 0
    total_actions_executed: int = 0

    # Success
    success_rate: float = 0.0           # Actions that succeeded
    user_satisfaction: float = 0.0      # From feedback
    task_completion_rate: float = 0.0   # Did user goal get met?

    # Performance
    avg_latency_ms: float = 0.0
    p95_latency_ms: float = 0.0

    # Issues
    error_count: int = 0
    confusion_count: int = 0            # User had to clarify
    override_count: int = 0             # User corrected the action

    # Evolution
    generations: int = 0                # How many times evolved
    fitness_score: float = 0.0          # Composite fitness
    last_evolved: datetime | None = None

    # Per-action breakdown
    action_metrics: dict[str, ActionMetrics] = field(default_factory=dict)


@dataclass
class ActionMetrics:
    """Metrics for a single action."""
    invocation_count: int = 0
    success_count: int = 0
    failure_count: int = 0
    avg_latency_ms: float = 0.0
    last_invoked: datetime | None = None


@dataclass
class Behavior:
    """Complete behavior definition."""

    # Identity
    behavior_id: str                    # "voice_assistant", "dungeon_master"
    name: str                           # Human-readable name
    description: str                    # What this behavior enables
    version: str = "1.0.0"

    # Classification
    tier: BehaviorTier = BehaviorTier.APPLICATION
    status: BehaviorStatus = BehaviorStatus.DRAFT

    # Core definition
    actions: list[Action] = field(default_factory=list)
    triggers: list[Trigger] = field(default_factory=list)
    prompts: BehaviorPrompts | None = None
    constraints: BehaviorConstraints = field(default_factory=BehaviorConstraints)

    # Context for LLM
    domain_context: str = ""            # Background knowledge for this domain
    personality_guidance: str = ""      # How personality should manifest

    # Activation rules
    activation_scope: ActivationScope = ActivationScope.GLOBAL
    activation_conditions: list[str] = field(default_factory=list)

    # Dependencies
    requires_behaviors: list[str] = field(default_factory=list)  # Must also be active
    conflicts_with: list[str] = field(default_factory=list)      # Cannot coexist
    extends: str | None = None          # Parent behavior to extend

    # Metadata
    author: str = "system"
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)
    metrics: BehaviorMetrics = field(default_factory=BehaviorMetrics)

    # Evolution
    is_evolvable: bool = True           # Can be improved automatically
    evolution_config: dict[str, Any] = field(default_factory=dict)
    parent_behavior_id: str | None = None  # If evolved from another

    # Testing
    test_cases: list[BehaviorTestCase] = field(default_factory=list)
    test_results: TestResults | None = None


@dataclass
class BehaviorTestCase:
    """A test case for validating behavior."""

    test_id: str
    name: str
    description: str

    # Input
    user_query: str
    context: dict[str, Any] = field(default_factory=dict)

    # Expected outcomes (any match = pass)
    expected_actions: list[str] = field(default_factory=list)      # Action names
    expected_action_args: dict[str, Any] | None = None             # Specific args
    expected_response_contains: list[str] = field(default_factory=list)
    expected_response_excludes: list[str] = field(default_factory=list)

    # Negative tests
    forbidden_actions: list[str] = field(default_factory=list)

    # Metadata
    priority: str = "medium"            # high, medium, low
    tags: list[str] = field(default_factory=list)


@dataclass
class TestResults:
    """Results from running behavior tests."""

    total_tests: int = 0
    passed: int = 0
    failed: int = 0
    skipped: int = 0

    pass_rate: float = 0.0

    # Detailed results
    test_outcomes: dict[str, TestOutcome] = field(default_factory=dict)

    run_at: datetime = field(default_factory=datetime.now)
    duration_seconds: float = 0.0


@dataclass
class TestOutcome:
    """Outcome of a single test."""
    test_id: str
    passed: bool
    actual_action: str | None = None
    actual_response: str | None = None
    failure_reason: str | None = None
    latency_ms: float = 0.0
```

---

## 2. Behavior Registry

The registry manages all behaviors across tiers.

```python
from typing import Iterator
from pathlib import Path
import json


class BehaviorRegistry:
    """Central registry for all behaviors."""

    def __init__(
        self,
        core_behaviors_path: Path | None = None,
        addon_behaviors_path: Path | None = None,
        storage_path: Path | None = None,
    ):
        self._behaviors: dict[str, Behavior] = {}
        self._by_tier: dict[BehaviorTier, list[str]] = {t: [] for t in BehaviorTier}
        self._storage_path = storage_path

        # Load core behaviors from package
        if core_behaviors_path:
            self._load_from_directory(core_behaviors_path, BehaviorTier.CORE)

        # Load add-on behaviors
        if addon_behaviors_path:
            self._load_from_directory(addon_behaviors_path, BehaviorTier.ADDON)

        # Load generated/custom behaviors from storage
        if storage_path:
            self._load_from_storage()

    # =========================================================================
    # Registration
    # =========================================================================

    def register(self, behavior: Behavior) -> None:
        """Register a behavior."""
        self._behaviors[behavior.behavior_id] = behavior
        self._by_tier[behavior.tier].append(behavior.behavior_id)

    def register_from_application(
        self,
        behavior: Behavior,
        application_id: str,
    ) -> None:
        """Register a behavior from a host application."""
        behavior.tier = BehaviorTier.APPLICATION
        behavior.author = f"app:{application_id}"
        self.register(behavior)

    def unregister(self, behavior_id: str) -> bool:
        """Remove a behavior."""
        if behavior_id not in self._behaviors:
            return False

        behavior = self._behaviors[behavior_id]
        self._by_tier[behavior.tier].remove(behavior_id)
        del self._behaviors[behavior_id]
        return True

    # =========================================================================
    # Lookup
    # =========================================================================

    def get(self, behavior_id: str) -> Behavior | None:
        """Get a behavior by ID."""
        return self._behaviors.get(behavior_id)

    def get_by_tier(self, tier: BehaviorTier) -> list[Behavior]:
        """Get all behaviors of a tier."""
        return [self._behaviors[bid] for bid in self._by_tier[tier]]

    def get_active(self) -> list[Behavior]:
        """Get all active behaviors."""
        return [b for b in self._behaviors.values() if b.status == BehaviorStatus.ACTIVE]

    def list_all(self) -> list[str]:
        """List all behavior IDs."""
        return list(self._behaviors.keys())

    def find_by_trigger(self, query: str, context: dict) -> list[Behavior]:
        """Find behaviors whose triggers match."""
        matches = []
        for behavior in self.get_active():
            for trigger in behavior.triggers:
                if self._trigger_matches(trigger, query, context):
                    matches.append(behavior)
                    break
        return sorted(matches, key=lambda b: max(t.priority for t in b.triggers), reverse=True)

    # =========================================================================
    # Persistence
    # =========================================================================

    def save_behavior(self, behavior: Behavior) -> None:
        """Persist a behavior to storage."""
        if not self._storage_path:
            return

        path = self._storage_path / f"{behavior.behavior_id}.json"
        with open(path, "w") as f:
            json.dump(self._serialize_behavior(behavior), f, indent=2)

    def _load_from_storage(self) -> None:
        """Load behaviors from storage directory."""
        if not self._storage_path or not self._storage_path.exists():
            return

        for path in self._storage_path.glob("*.json"):
            with open(path) as f:
                data = json.load(f)
                behavior = self._deserialize_behavior(data)
                self.register(behavior)

    # =========================================================================
    # Helpers
    # =========================================================================

    def _trigger_matches(self, trigger: Trigger, query: str, context: dict) -> bool:
        """Check if a trigger matches the query/context."""
        # Check exclusions first
        for pattern in trigger.exclusion_patterns:
            if self._pattern_matches(pattern, query):
                return False

        # Check positive patterns (any match = True)
        for pattern in trigger.keyword_patterns:
            if self._pattern_matches(pattern, query):
                return True

        for intent in trigger.intent_categories:
            if context.get("intent") == intent:
                return True

        for condition in trigger.context_conditions:
            if self._evaluate_condition(condition, context):
                return True

        # Semantic patterns require LLM - handled by activation engine
        return len(trigger.semantic_patterns) > 0
```

---

## 3. Behavior Activation Engine

Decides which behaviors are active for a given context.

```python
from dataclasses import dataclass


@dataclass
class ActivationContext:
    """Full context for behavior activation decisions."""

    # Application context
    application_id: str
    application_features: set[str]

    # User context
    user_id: str
    user_permissions: set[str]
    user_preferences: dict[str, Any]

    # Session context
    session_id: str
    conversation_history: list[dict]
    active_persona: str | None = None

    # Situational context
    current_query: str = ""
    detected_intent: str | None = None
    detected_entities: list[str] = field(default_factory=list)
    time_of_day: str = "day"            # morning, day, evening, night
    urgency: str = "normal"             # low, normal, high, urgent


@dataclass
class ActiveBehaviorSet:
    """The set of behaviors active for a context."""

    # Primary behavior (handles main decision)
    primary: Behavior | None = None

    # Supporting behaviors (provide additional actions)
    supporting: list[Behavior] = field(default_factory=list)

    # All available actions across active behaviors
    available_actions: list[Action] = field(default_factory=list)

    # Combined constraints
    combined_constraints: BehaviorConstraints = field(default_factory=BehaviorConstraints)

    # Metadata
    activation_reason: str = ""


class BehaviorActivationEngine:
    """Determines which behaviors are active for a given context."""

    def __init__(
        self,
        registry: BehaviorRegistry,
        llm: LLMProvider,
    ):
        self.registry = registry
        self.llm = llm

    async def get_active_behaviors(
        self,
        context: ActivationContext,
    ) -> ActiveBehaviorSet:
        """Determine active behaviors for a context."""

        result = ActiveBehaviorSet()
        candidates: list[tuple[Behavior, float]] = []  # (behavior, score)

        for behavior in self.registry.get_active():
            # Check hard constraints
            if not self._passes_constraints(behavior, context):
                continue

            # Calculate activation score
            score = await self._calculate_activation_score(behavior, context)
            if score > 0:
                candidates.append((behavior, score))

        if not candidates:
            return result

        # Sort by score
        candidates.sort(key=lambda x: x[1], reverse=True)

        # Primary behavior is highest scoring
        result.primary = candidates[0][0]

        # Supporting behaviors are compatible ones
        for behavior, score in candidates[1:]:
            if self._is_compatible(behavior, result.primary):
                result.supporting.append(behavior)

        # Collect all available actions
        result.available_actions = self._collect_actions(result)

        # Combine constraints
        result.combined_constraints = self._combine_constraints(result)

        return result

    def _passes_constraints(self, behavior: Behavior, context: ActivationContext) -> bool:
        """Check if behavior passes hard constraints."""
        constraints = behavior.constraints

        # Application constraint
        if constraints.allowed_applications is not None:
            if context.application_id not in constraints.allowed_applications:
                return False

        # User constraint
        if constraints.allowed_users is not None:
            if context.user_id not in constraints.allowed_users:
                return False

        # Permission constraint
        for perm in constraints.required_permissions:
            if perm not in context.user_permissions:
                return False

        return True

    async def _calculate_activation_score(
        self,
        behavior: Behavior,
        context: ActivationContext,
    ) -> float:
        """Calculate how strongly a behavior should activate."""
        score = 0.0

        for trigger in behavior.triggers:
            trigger_score = 0.0

            # Keyword matches
            for pattern in trigger.keyword_patterns:
                if self._pattern_matches(pattern, context.current_query):
                    trigger_score = max(trigger_score, 0.7)

            # Intent matches
            if context.detected_intent in trigger.intent_categories:
                trigger_score = max(trigger_score, 0.8)

            # Semantic matching (LLM)
            if trigger.semantic_patterns:
                semantic_score = await self._semantic_match(
                    trigger.semantic_patterns,
                    context.current_query,
                    context,
                )
                trigger_score = max(trigger_score, semantic_score)

            # Apply priority weighting
            weighted_score = trigger_score * (trigger.priority / 100)
            score = max(score, weighted_score)

        return score

    async def _semantic_match(
        self,
        patterns: list[str],
        query: str,
        context: ActivationContext,
    ) -> float:
        """Use LLM to evaluate semantic trigger match."""
        prompt = f"""Evaluate if this query matches any of these trigger patterns.

QUERY: {query}

TRIGGER PATTERNS:
{chr(10).join(f"- {p}" for p in patterns)}

CONVERSATION CONTEXT:
{self._format_history(context.conversation_history[-3:])}

Rate the match from 0.0 (no match) to 1.0 (strong match).
Output only a number."""

        response = await self.llm.chat([{"role": "user", "content": prompt}])
        try:
            return float(response.content.strip())
        except ValueError:
            return 0.0
```

---

## 4. Behavior Testing Framework

Comprehensive testing for behaviors.

```python
from typing import AsyncGenerator
import asyncio


class BehaviorTestRunner:
    """Runs tests for behaviors."""

    def __init__(
        self,
        llm: LLMProvider,
        memory: MemoryProvider,
        registry: BehaviorRegistry,
    ):
        self.llm = llm
        self.memory = memory
        self.registry = registry

    async def run_tests(
        self,
        behavior: Behavior,
        test_filter: list[str] | None = None,
    ) -> TestResults:
        """Run all tests for a behavior."""

        results = TestResults()
        test_cases = behavior.test_cases

        if test_filter:
            test_cases = [t for t in test_cases if t.test_id in test_filter]

        results.total_tests = len(test_cases)

        start_time = datetime.now()

        for test_case in test_cases:
            outcome = await self._run_single_test(behavior, test_case)
            results.test_outcomes[test_case.test_id] = outcome

            if outcome.passed:
                results.passed += 1
            else:
                results.failed += 1

        results.duration_seconds = (datetime.now() - start_time).total_seconds()
        results.pass_rate = results.passed / results.total_tests if results.total_tests > 0 else 0.0

        # Store results
        behavior.test_results = results

        return results

    async def _run_single_test(
        self,
        behavior: Behavior,
        test_case: BehaviorTestCase,
    ) -> TestOutcome:
        """Run a single test case."""

        outcome = TestOutcome(test_id=test_case.test_id, passed=False)
        start_time = datetime.now()

        try:
            # Build decision prompt
            decision_prompt = self._build_test_prompt(behavior, test_case)

            # Get LLM decision
            response = await self.llm.chat([
                {"role": "system", "content": decision_prompt},
                {"role": "user", "content": test_case.user_query},
            ])

            # Parse action from response
            actual_action, actual_response = self._parse_response(response.content)
            outcome.actual_action = actual_action
            outcome.actual_response = actual_response

            # Validate
            outcome.passed = self._validate_outcome(test_case, actual_action, actual_response)

            if not outcome.passed:
                outcome.failure_reason = self._explain_failure(test_case, actual_action, actual_response)

        except Exception as e:
            outcome.failure_reason = str(e)

        outcome.latency_ms = (datetime.now() - start_time).total_seconds() * 1000
        return outcome

    def _validate_outcome(
        self,
        test_case: BehaviorTestCase,
        actual_action: str | None,
        actual_response: str | None,
    ) -> bool:
        """Validate test outcome against expectations."""

        # Check forbidden actions
        if actual_action in test_case.forbidden_actions:
            return False

        # Check expected actions
        if test_case.expected_actions:
            if actual_action not in test_case.expected_actions:
                return False

        # Check response content
        if actual_response:
            for phrase in test_case.expected_response_contains:
                if phrase.lower() not in actual_response.lower():
                    return False

            for phrase in test_case.expected_response_excludes:
                if phrase.lower() in actual_response.lower():
                    return False

        return True

    async def generate_test_cases(
        self,
        behavior: Behavior,
        count: int = 10,
    ) -> list[BehaviorTestCase]:
        """Auto-generate test cases for a behavior."""

        prompt = f"""Generate {count} test cases for this behavior.

BEHAVIOR: {behavior.name}
DESCRIPTION: {behavior.description}

AVAILABLE ACTIONS:
{self._format_actions(behavior.actions)}

TRIGGERS:
{self._format_triggers(behavior.triggers)}

Generate diverse test cases covering:
1. Happy path - normal usage
2. Edge cases - unusual but valid
3. Negative tests - things that should NOT trigger this behavior
4. Boundary tests - limits and constraints

For each test case, provide:
- test_id: unique identifier
- name: brief name
- user_query: what the user says
- expected_actions: which action(s) should trigger
- expected_response_contains: key phrases in response
- forbidden_actions: actions that should NOT trigger

Output as JSON array."""

        response = await self.llm.chat([{"role": "user", "content": prompt}])

        # Parse and create test cases
        test_data = json.loads(response.content)
        return [BehaviorTestCase(**tc) for tc in test_data]


class BehaviorValidator:
    """Validates behavior definitions."""

    def validate(self, behavior: Behavior) -> list[ValidationIssue]:
        """Validate a behavior definition."""
        issues = []

        # Required fields
        if not behavior.behavior_id:
            issues.append(ValidationIssue("error", "behavior_id is required"))

        if not behavior.actions:
            issues.append(ValidationIssue("warning", "No actions defined"))

        if not behavior.triggers:
            issues.append(ValidationIssue("warning", "No triggers defined"))

        if not behavior.prompts:
            issues.append(ValidationIssue("error", "Prompts are required"))

        # Action validation
        for action in behavior.actions:
            if not action.description:
                issues.append(ValidationIssue(
                    "warning",
                    f"Action '{action.name}' has no description"
                ))

            if not action.examples:
                issues.append(ValidationIssue(
                    "info",
                    f"Action '{action.name}' has no examples - may reduce accuracy"
                ))

        # Test coverage
        if not behavior.test_cases:
            issues.append(ValidationIssue(
                "warning",
                "No test cases defined"
            ))
        else:
            action_names = {a.name for a in behavior.actions}
            tested_actions = set()
            for tc in behavior.test_cases:
                tested_actions.update(tc.expected_actions)

            untested = action_names - tested_actions
            if untested:
                issues.append(ValidationIssue(
                    "warning",
                    f"Actions without test coverage: {untested}"
                ))

        return issues


@dataclass
class ValidationIssue:
    """A validation issue found."""
    severity: str  # error, warning, info
    message: str
```

---

## 5. Behavior Evolution Engine

Automatically improves behaviors using evolutionary techniques.

```python
from typing import Callable


@dataclass
class EvolutionConfig:
    """Configuration for behavior evolution."""

    # Population settings
    population_size: int = 6
    generations: int = 5

    # Selection
    tournament_size: int = 3
    elitism_count: int = 1

    # Mutation
    mutation_rate: float = 0.3
    crossover_rate: float = 0.5

    # Fitness
    min_test_cases: int = 20
    train_test_split: float = 0.8
    overfitting_threshold: float = 0.1

    # Safety
    require_manual_approval: bool = True
    max_generations_without_improvement: int = 3


class BehaviorEvolutionEngine:
    """Evolves behaviors using genetic algorithms."""

    def __init__(
        self,
        llm: LLMProvider,
        test_runner: BehaviorTestRunner,
        registry: BehaviorRegistry,
    ):
        self.llm = llm
        self.test_runner = test_runner
        self.registry = registry

    async def evolve(
        self,
        behavior: Behavior,
        config: EvolutionConfig | None = None,
    ) -> BehaviorEvolutionResult:
        """Evolve a behavior to improve its performance."""

        config = config or EvolutionConfig()

        # Ensure sufficient test cases
        if len(behavior.test_cases) < config.min_test_cases:
            new_tests = await self.test_runner.generate_test_cases(
                behavior,
                config.min_test_cases - len(behavior.test_cases)
            )
            behavior.test_cases.extend(new_tests)

        # Split train/test
        train_cases, holdout_cases = self._split_test_cases(
            behavior.test_cases,
            config.train_test_split,
        )

        # Initialize population
        population = await self._initialize_population(behavior, config)

        best_behavior = behavior
        best_fitness = await self._evaluate_fitness(behavior, train_cases)
        generations_without_improvement = 0

        for gen in range(config.generations):
            # Evaluate all
            fitness_scores = {}
            for b in population:
                fitness_scores[b.behavior_id] = await self._evaluate_fitness(b, train_cases)

            # Track best
            gen_best_id = max(fitness_scores, key=fitness_scores.get)
            gen_best_fitness = fitness_scores[gen_best_id]

            if gen_best_fitness > best_fitness:
                best_behavior = self._get_by_id(population, gen_best_id)
                best_fitness = gen_best_fitness
                generations_without_improvement = 0
            else:
                generations_without_improvement += 1

            if generations_without_improvement >= config.max_generations_without_improvement:
                break  # Converged

            # Create next generation
            population = await self._create_next_generation(
                population,
                fitness_scores,
                config,
            )

        # Validate on holdout
        holdout_fitness = await self._evaluate_fitness(best_behavior, holdout_cases)
        train_fitness = await self._evaluate_fitness(best_behavior, train_cases)

        overfitting_gap = train_fitness - holdout_fitness

        return BehaviorEvolutionResult(
            original_behavior=behavior,
            evolved_behavior=best_behavior,
            original_fitness=await self._evaluate_fitness(behavior, behavior.test_cases),
            evolved_fitness=holdout_fitness,
            overfitting_gap=overfitting_gap,
            generations_run=gen + 1,
            approved=overfitting_gap < config.overfitting_threshold,
        )

    async def _initialize_population(
        self,
        base: Behavior,
        config: EvolutionConfig,
    ) -> list[Behavior]:
        """Create initial population."""
        population = [base]  # Include original

        for i in range(config.population_size - 1):
            mutant = await self._mutate(base, f"{base.behavior_id}_m{i}")
            population.append(mutant)

        return population

    async def _mutate(self, behavior: Behavior, new_id: str) -> Behavior:
        """Create a mutation of a behavior."""

        # Deep copy
        mutant = self._deep_copy(behavior)
        mutant.behavior_id = new_id
        mutant.parent_behavior_id = behavior.behavior_id
        mutant.version = self._increment_version(behavior.version)

        # Choose mutation type
        mutation_type = random.choice([
            "prompt_refinement",
            "action_description",
            "trigger_patterns",
            "examples",
            "constraints",
        ])

        if mutation_type == "prompt_refinement":
            mutant.prompts = await self._mutate_prompts(behavior.prompts)

        elif mutation_type == "action_description":
            for action in mutant.actions:
                action.description = await self._improve_description(action)

        elif mutation_type == "trigger_patterns":
            for trigger in mutant.triggers:
                trigger.semantic_patterns = await self._expand_patterns(trigger)

        elif mutation_type == "examples":
            for action in mutant.actions:
                new_examples = await self._generate_examples(action)
                action.examples.extend(new_examples)

        return mutant

    async def _mutate_prompts(self, prompts: BehaviorPrompts) -> BehaviorPrompts:
        """Mutate prompt templates."""

        mutation_prompt = f"""Improve this decision prompt to be more effective.

CURRENT PROMPT:
{prompts.decision_prompt}

Suggest an improved version that:
1. Is clearer about when to use each action
2. Better handles edge cases
3. Maintains the same actions but improves decision quality

Output only the improved prompt."""

        response = await self.llm.chat([{"role": "user", "content": mutation_prompt}])

        return BehaviorPrompts(
            decision_prompt=response.content,
            synthesis_prompt=prompts.synthesis_prompt,
            version=self._increment_version(prompts.version),
            parent_version=prompts.version,
        )

    async def _evaluate_fitness(
        self,
        behavior: Behavior,
        test_cases: list[BehaviorTestCase],
    ) -> float:
        """Calculate fitness score for a behavior."""

        # Run tests
        temp_behavior = self._deep_copy(behavior)
        temp_behavior.test_cases = test_cases
        results = await self.test_runner.run_tests(temp_behavior)

        # Fitness = pass rate with penalty for slow performance
        fitness = results.pass_rate

        # Penalty for slow behaviors
        avg_latency = sum(
            o.latency_ms for o in results.test_outcomes.values()
        ) / len(results.test_outcomes) if results.test_outcomes else 0

        if avg_latency > 5000:  # > 5 seconds
            fitness *= 0.9

        return fitness


@dataclass
class BehaviorEvolutionResult:
    """Result of behavior evolution."""
    original_behavior: Behavior
    evolved_behavior: Behavior
    original_fitness: float
    evolved_fitness: float
    overfitting_gap: float
    generations_run: int
    approved: bool  # Passed overfitting check
```

---

## 6. Behavior Architect (Meta-Behavior)

A behavior that creates and improves other behaviors!

```python
# This is itself a behavior definition
BEHAVIOR_ARCHITECT = Behavior(
    behavior_id="behavior_architect",
    name="Behavior Architect",
    description="""A meta-behavior that can create, research, test, and evolve other behaviors.

    This behavior enables the agent to:
    - Research domains and create new behaviors
    - Analyze failing behaviors and improve them
    - Run tests and evolution on behaviors
    - Monitor behavior health and suggest improvements
    """,

    tier=BehaviorTier.CORE,
    status=BehaviorStatus.ACTIVE,

    actions=[
        Action(
            name="research_domain",
            description="Research a domain to understand what a behavior should do",
            parameters={
                "domain": ActionParameter("domain", "Domain to research", "string"),
                "depth": ActionParameter("depth", "How deep to research", "string",
                                         default="medium", enum_values=["shallow", "medium", "deep"]),
            },
            examples=[
                ActionExample(
                    user_query="I need a behavior for managing a restaurant",
                    action_call={"name": "research_domain", "args": {"domain": "restaurant_management", "depth": "deep"}},
                    expected_outcome="Research report on restaurant management domain",
                ),
            ],
        ),
        Action(
            name="create_behavior",
            description="Create a new behavior definition from research",
            parameters={
                "domain": ActionParameter("domain", "Domain for the behavior"),
                "name": ActionParameter("name", "Name for the behavior"),
                "based_on": ActionParameter("based_on", "Existing behavior to extend", required=False),
            },
            examples=[
                ActionExample(
                    user_query="Create a restaurant manager behavior based on the research",
                    action_call={"name": "create_behavior", "args": {
                        "domain": "restaurant_management",
                        "name": "restaurant_manager",
                    }},
                    expected_outcome="New behavior definition created",
                ),
            ],
        ),
        Action(
            name="generate_tests",
            description="Generate test cases for a behavior",
            parameters={
                "behavior_id": ActionParameter("behavior_id", "Behavior to generate tests for"),
                "count": ActionParameter("count", "Number of tests to generate", "int", default=20),
            },
        ),
        Action(
            name="run_tests",
            description="Run tests for a behavior",
            parameters={
                "behavior_id": ActionParameter("behavior_id", "Behavior to test"),
            },
        ),
        Action(
            name="analyze_failures",
            description="Analyze test failures and suggest fixes",
            parameters={
                "behavior_id": ActionParameter("behavior_id", "Behavior with failures"),
            },
        ),
        Action(
            name="evolve_behavior",
            description="Run evolutionary improvement on a behavior",
            parameters={
                "behavior_id": ActionParameter("behavior_id", "Behavior to evolve"),
                "generations": ActionParameter("generations", "Max generations", "int", default=5),
            },
        ),
        Action(
            name="compare_behaviors",
            description="Compare two behaviors on the same test set",
            parameters={
                "behavior_a": ActionParameter("behavior_a", "First behavior ID"),
                "behavior_b": ActionParameter("behavior_b", "Second behavior ID"),
            },
        ),
        Action(
            name="promote_behavior",
            description="Promote a behavior to a higher status",
            parameters={
                "behavior_id": ActionParameter("behavior_id", "Behavior to promote"),
                "new_status": ActionParameter("new_status", "Target status",
                                              enum_values=["testing", "staging", "active"]),
            },
            requires_confirmation=True,
        ),
        Action(
            name="retire_behavior",
            description="Retire a behavior that's no longer needed",
            parameters={
                "behavior_id": ActionParameter("behavior_id", "Behavior to retire"),
                "reason": ActionParameter("reason", "Why retiring"),
            },
            requires_confirmation=True,
        ),
    ],

    triggers=[
        Trigger(
            name="behavior_management",
            semantic_patterns=[
                "create a new behavior",
                "make a behavior for",
                "improve the behavior",
                "why is this behavior failing",
                "test the behavior",
                "evolve this behavior",
            ],
            priority=90,  # High priority for meta-operations
        ),
    ],

    prompts=BehaviorPrompts(
        decision_prompt=BEHAVIOR_ARCHITECT_DECISION_PROMPT,
        synthesis_prompt=BEHAVIOR_ARCHITECT_SYNTHESIS_PROMPT,
    ),

    constraints=BehaviorConstraints(
        requires_user_confirmation=["promote_behavior", "retire_behavior"],
        required_permissions=["behavior:manage"],
    ),
)


# Implementation of the Behavior Architect's actions
class BehaviorArchitectExecutor:
    """Executes Behavior Architect actions."""

    def __init__(
        self,
        llm: LLMProvider,
        memory: MemoryProvider,
        search: SearchProvider,
        registry: BehaviorRegistry,
        test_runner: BehaviorTestRunner,
        evolution_engine: BehaviorEvolutionEngine,
    ):
        self.llm = llm
        self.memory = memory
        self.search = search
        self.registry = registry
        self.test_runner = test_runner
        self.evolution_engine = evolution_engine

    async def research_domain(
        self,
        domain: str,
        depth: str = "medium",
    ) -> DomainResearchResult:
        """Research a domain to understand behavior requirements."""

        # 1. Web search for domain knowledge
        search_queries = [
            f"{domain} best practices",
            f"{domain} common tasks",
            f"{domain} user workflows",
            f"{domain} software features",
        ]

        if depth == "deep":
            search_queries.extend([
                f"{domain} edge cases",
                f"{domain} common mistakes",
                f"{domain} expert tips",
            ])

        research_results = []
        for query in search_queries:
            results = await self.search.search(query)
            research_results.extend(results[:3])

        # 2. Synthesize into structured knowledge
        synthesis_prompt = f"""Analyze this research about {domain} and extract:

1. CORE TASKS: What are the main things users do in this domain?
2. ACTIONS NEEDED: What specific actions should a behavior support?
3. TRIGGERS: What phrases/intents should activate this behavior?
4. CONSTRAINTS: What rules or limits apply?
5. DOMAIN KNOWLEDGE: What background context is needed?

RESEARCH:
{self._format_research(research_results)}

Output as structured JSON."""

        response = await self.llm.chat([{"role": "user", "content": synthesis_prompt}])

        return DomainResearchResult.from_json(response.content)

    async def create_behavior(
        self,
        domain: str,
        name: str,
        research: DomainResearchResult | None = None,
        based_on: str | None = None,
    ) -> Behavior:
        """Create a new behavior from research."""

        # Get research if not provided
        if not research:
            research = await self.research_domain(domain, depth="deep")

        # Get base behavior if extending
        base = None
        if based_on:
            base = self.registry.get(based_on)

        # Generate behavior definition
        generation_prompt = f"""Create a behavior definition for: {name}

DOMAIN RESEARCH:
{research.to_summary()}

{"EXTENDING BASE BEHAVIOR: " + base.name if base else ""}

Generate a complete behavior definition including:
1. Actions with parameters and examples
2. Triggers with semantic and keyword patterns
3. Decision prompt template
4. Constraints and guidelines
5. At least 10 test cases

Output as JSON matching the Behavior schema."""

        response = await self.llm.chat([{"role": "user", "content": generation_prompt}])

        behavior_data = json.loads(response.content)
        behavior = Behavior(**behavior_data)

        # Set metadata
        behavior.tier = BehaviorTier.GENERATED
        behavior.status = BehaviorStatus.DRAFT
        behavior.author = "behavior_architect"
        behavior.created_at = datetime.now()

        if base:
            behavior.extends = base.behavior_id

        # Register
        self.registry.register(behavior)
        self.registry.save_behavior(behavior)

        return behavior

    async def analyze_failures(self, behavior_id: str) -> FailureAnalysis:
        """Analyze test failures and suggest fixes."""

        behavior = self.registry.get(behavior_id)
        if not behavior or not behavior.test_results:
            raise ValueError(f"No test results for {behavior_id}")

        # Collect failures
        failures = [
            (tid, outcome)
            for tid, outcome in behavior.test_results.test_outcomes.items()
            if not outcome.passed
        ]

        if not failures:
            return FailureAnalysis(no_failures=True)

        # Analyze patterns
        analysis_prompt = f"""Analyze these test failures for the behavior "{behavior.name}":

BEHAVIOR DESCRIPTION:
{behavior.description}

ACTIONS:
{self._format_actions(behavior.actions)}

FAILURES:
{self._format_failures(failures, behavior.test_cases)}

Identify:
1. PATTERNS: Common themes in failures
2. ROOT CAUSES: Why the behavior fails in these cases
3. FIXES: Specific changes to prompts, actions, or triggers that would fix these

Output as structured analysis."""

        response = await self.llm.chat([{"role": "user", "content": analysis_prompt}])

        return FailureAnalysis.from_llm_response(response.content)
```

---

## 7. Behavior Configuration & Scoping

How applications configure which behaviors are available.

```python
@dataclass
class BehaviorConfiguration:
    """Application-level behavior configuration."""

    # Application identity
    application_id: str

    # Enabled behavior tiers
    enabled_tiers: set[BehaviorTier] = field(default_factory=lambda: {
        BehaviorTier.CORE,
        BehaviorTier.APPLICATION,
    })

    # Explicit behavior enables/disables
    enabled_behaviors: set[str] = field(default_factory=set)     # Always enable these
    disabled_behaviors: set[str] = field(default_factory=set)    # Never enable these

    # Feature flags
    allow_generated_behaviors: bool = False   # Allow auto-generated behaviors
    allow_experimental: bool = False          # Allow experimental status
    allow_behavior_evolution: bool = False    # Allow self-improvement

    # User-specific overrides
    user_behavior_overrides: dict[str, UserBehaviorConfig] = field(default_factory=dict)

    # Default constraints to apply
    global_constraints: BehaviorConstraints = field(default_factory=BehaviorConstraints)


@dataclass
class UserBehaviorConfig:
    """User-specific behavior configuration."""

    user_id: str

    # Permissions
    allowed_behaviors: set[str] | None = None   # None = all allowed
    blocked_behaviors: set[str] = field(default_factory=set)

    # Preferences
    preferred_behaviors: list[str] = field(default_factory=list)  # Prioritize these

    # Limits
    max_actions_per_minute: int = 60
    allowed_action_types: set[str] | None = None


class BehaviorConfigManager:
    """Manages behavior configuration for an application."""

    def __init__(
        self,
        registry: BehaviorRegistry,
        config: BehaviorConfiguration,
    ):
        self.registry = registry
        self.config = config

    def get_available_behaviors(
        self,
        user_id: str,
    ) -> list[Behavior]:
        """Get behaviors available for a user."""

        available = []
        user_config = self.config.user_behavior_overrides.get(user_id)

        for behavior in self.registry.get_active():
            # Check tier
            if behavior.tier not in self.config.enabled_tiers:
                continue

            # Check tier-specific flags
            if behavior.tier == BehaviorTier.GENERATED and not self.config.allow_generated_behaviors:
                continue

            if behavior.status == BehaviorStatus.EXPERIMENTAL and not self.config.allow_experimental:
                continue

            # Check explicit enables/disables
            if behavior.behavior_id in self.config.disabled_behaviors:
                continue

            # Check user-specific
            if user_config:
                if user_config.allowed_behaviors is not None:
                    if behavior.behavior_id not in user_config.allowed_behaviors:
                        continue

                if behavior.behavior_id in user_config.blocked_behaviors:
                    continue

            available.append(behavior)

        return available

    def is_action_allowed(
        self,
        user_id: str,
        behavior_id: str,
        action_name: str,
    ) -> bool:
        """Check if a specific action is allowed."""

        behavior = self.registry.get(behavior_id)
        if not behavior:
            return False

        # Check behavior constraints
        if action_name in behavior.constraints.blocked_actions:
            return False

        # Check global constraints
        if action_name in self.config.global_constraints.blocked_actions:
            return False

        # Check user permissions
        action = next((a for a in behavior.actions if a.name == action_name), None)
        if action:
            for perm in action.required_permissions:
                if not self._user_has_permission(user_id, perm):
                    return False

        return True
```

---

## 8. Complete System Flow

### Creating a New Behavior (via Behavior Architect)

```
User: "Create a behavior for managing a D&D game"
                    │
                    ▼
┌─────────────────────────────────────────────────────────────────────┐
│ 1. TRIGGER MATCHING                                                  │
│    - Query matches "behavior_architect" trigger                     │
│    - Behavior Architect becomes primary behavior                    │
└─────────────────────────────────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────────────────────────┐
│ 2. ACTION SELECTION                                                  │
│    - LLM evaluates available actions                                │
│    - Selects: research_domain → create_behavior                     │
└─────────────────────────────────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────────────────────────┐
│ 3. RESEARCH PHASE                                                    │
│    - Web search: "D&D game master tasks", "tabletop RPG software"   │
│    - Memory search: past RPG-related conversations                  │
│    - Synthesize into DomainResearchResult                           │
└─────────────────────────────────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────────────────────────┐
│ 4. BEHAVIOR GENERATION                                               │
│    - LLM creates behavior definition from research                  │
│    - Generates: actions, triggers, prompts, test cases              │
│    - Status: DRAFT, Tier: GENERATED                                 │
└─────────────────────────────────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────────────────────────┐
│ 5. TESTING                                                           │
│    - Run generated test cases                                        │
│    - Generate additional edge case tests                            │
│    - If pass rate < 80%: analyze failures, iterate                  │
└─────────────────────────────────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────────────────────────┐
│ 6. EVOLUTION (if enabled)                                            │
│    - Run evolutionary improvement                                    │
│    - Test on holdout set to prevent overfitting                     │
│    - Track fitness improvements                                      │
└─────────────────────────────────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────────────────────────┐
│ 7. PROMOTION                                                         │
│    - If tests pass: promote to TESTING → STAGING → ACTIVE           │
│    - Store in registry                                               │
│    - Available for use                                               │
└─────────────────────────────────────────────────────────────────────┘
```

### Using Behaviors at Runtime

```
User: "Roll initiative for the party"
                    │
                    ▼
┌─────────────────────────────────────────────────────────────────────┐
│ 1. BUILD ACTIVATION CONTEXT                                          │
│    - Application: rpg_game                                          │
│    - User: doug (permissions: [game:master])                        │
│    - Session: campaign_123                                           │
│    - Query: "Roll initiative for the party"                         │
└─────────────────────────────────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────────────────────────┐
│ 2. BEHAVIOR ACTIVATION                                               │
│    - Filter by: user permissions, app config                        │
│    - Match triggers: "dungeon_master" triggers on "initiative"      │
│    - Score behaviors: dungeon_master = 0.92                         │
│    - Select primary: dungeon_master                                 │
│    - Add supporting: dice_roller, initiative_tracker                │
└─────────────────────────────────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────────────────────────┐
│ 3. BUILD DECISION PROMPT                                             │
│    - Inject personality: "You are a dramatic dungeon master..."    │
│    - Inject available actions: [roll_initiative, narrate, ...]     │
│    - Inject domain context: "Party is in combat with goblins..."   │
│    - Inject constraints: "Follow D&D 5e rules..."                   │
└─────────────────────────────────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────────────────────────┐
│ 4. LLM DECISION                                                      │
│    - Evaluates context + actions                                    │
│    - Selects: roll_initiative                                        │
│    - Args: {all_players: true, enemies: ["goblin_1", "goblin_2"]}   │
└─────────────────────────────────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────────────────────────┐
│ 5. ACTION EXECUTION                                                  │
│    - Execute via tool registry                                      │
│    - Result: {player_1: 18, player_2: 12, goblin_1: 7, ...}        │
└─────────────────────────────────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────────────────────────┐
│ 6. RESPONSE SYNTHESIS                                                │
│    - Use dungeon_master synthesis prompt                            │
│    - Dramatic narration: "Roll for initiative! The party springs   │
│      into action as steel clashes against goblin blades..."        │
└─────────────────────────────────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────────────────────────┐
│ 7. METRICS & LEARNING                                                │
│    - Track: behavior used, action taken, latency                    │
│    - Cognitive services: learn from this interaction                │
│    - Evolution data: feed into fitness calculations                 │
└─────────────────────────────────────────────────────────────────────┘
```

---

## 9. File Structure

```
draagon-ai/
├── src/draagon_ai/
│   ├── behaviors/
│   │   ├── __init__.py
│   │   ├── types.py                 # Behavior, Action, Trigger, etc.
│   │   ├── registry.py              # BehaviorRegistry
│   │   ├── activation.py            # BehaviorActivationEngine
│   │   ├── execution.py             # BehaviorExecutor
│   │   ├── testing.py               # BehaviorTestRunner, BehaviorValidator
│   │   ├── evolution.py             # BehaviorEvolutionEngine
│   │   ├── config.py                # BehaviorConfiguration
│   │   │
│   │   ├── core/                    # Built-in behaviors
│   │   │   ├── __init__.py
│   │   │   ├── behavior_architect.py   # Meta-behavior
│   │   │   ├── conversation.py         # Basic conversation
│   │   │   └── memory_management.py    # Memory operations
│   │   │
│   │   └── templates/               # Behavior templates for common patterns
│   │       ├── __init__.py
│   │       ├── assistant.py         # Template for assistants
│   │       ├── game_master.py       # Template for RPG GMs
│   │       └── npc.py               # Template for NPCs
│   │
│   ├── orchestration/               # Generic agent loop
│   │   ├── __init__.py
│   │   ├── agent.py                 # Agent class
│   │   ├── decision.py              # Decision engine
│   │   ├── synthesis.py             # Response synthesis
│   │   └── loop.py                  # Main agent loop
│   │
│   └── ... (existing modules)
│
├── tests/
│   └── behaviors/
│       ├── test_types.py
│       ├── test_registry.py
│       ├── test_activation.py
│       ├── test_testing.py
│       ├── test_evolution.py
│       └── test_behavior_architect.py
│
└── examples/
    ├── custom_behavior.py           # How to create a behavior
    ├── behavior_evolution.py        # How to evolve behaviors
    └── multi_behavior_agent.py      # Agent with multiple behaviors
```

---

## 10. Summary

### Key Design Principles

1. **Behaviors are Data** - Behaviors are serializable definitions, not code
2. **LLM-First** - All semantic decisions made by LLM, not hardcoded logic
3. **Testable by Default** - Every behavior has test cases
4. **Evolvable** - Behaviors can improve through automated evolution
5. **Meta-capable** - The system can create/improve itself
6. **Contextually Activated** - Right behavior for right situation
7. **Multi-tiered Trust** - Core vs addon vs generated with different trust levels
8. **Permission-aware** - Actions respect user permissions

### Tier Summary

| Tier | Source | Trust | Evolution | Approval |
|------|--------|-------|-----------|----------|
| CORE | Built into draagon-ai | Highest | Manual only | N/A |
| ADDON | Official packages | High | Semi-auto | Package maintainer |
| APPLICATION | Host app provides | Medium | App controls | App developer |
| GENERATED | Behavior Architect | Lower | Automatic | Requires promotion |
| EXPERIMENTAL | Testing | Lowest | Aggressive | Manual to promote |

### Evolution Path

```
DRAFT → TESTING → STAGING → ACTIVE → (DEPRECATED → RETIRED)
   ↑         │         │
   └─────────┴─────────┘
        Evolution
```
