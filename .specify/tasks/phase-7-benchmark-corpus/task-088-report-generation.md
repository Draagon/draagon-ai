# TASK-088: Report Generation (Markdown/CSV)

**Phase**: 7 (Production-Grade Retrieval Benchmark)
**Priority**: P0 (Required for documentation)
**Effort**: 1 day
**Status**: Pending
**Dependencies**: TASK-086 (BenchmarkRunner)

---

## Description

Generate comprehensive benchmark reports for documentation and analysis:
- Executive summary with key metrics
- Detailed per-query results
- Comparison tables vs baselines
- Category/difficulty breakdown
- Historical trend tracking

Reports should be publication-ready for documentation.

**Location:** `src/draagon_ai/testing/benchmarks/reports/`

---

## Acceptance Criteria

### Report Types
- [ ] Executive summary (1-page Markdown)
- [ ] Full detailed report (Markdown)
- [ ] Per-query results table (CSV)
- [ ] Comparison report (Markdown)
- [ ] JSON for programmatic access

### Executive Summary
- [ ] Overall aggregate score
- [ ] Key metrics (faithfulness, recall, etc.)
- [ ] Comparison to baselines (% improvement)
- [ ] Pass/fail status vs thresholds
- [ ] Run metadata (date, config, duration)

### Detailed Report
- [ ] All metrics with confidence intervals
- [ ] Per-category breakdown
- [ ] Per-difficulty breakdown
- [ ] High-variance queries highlighted
- [ ] Statistical significance

### Historical Tracking
- [ ] Append to results history file
- [ ] Trend charts data (JSON for plotting)
- [ ] Regression detection vs previous run

---

## Technical Notes

### Report Structure

```
benchmark_results/
├── 20260102_143022/           # Timestamped run folder
│   ├── summary.md             # Executive summary
│   ├── report.md              # Full detailed report
│   ├── results.json           # Machine-readable results
│   ├── per_query.csv          # Per-query breakdown
│   ├── comparison.md          # Baseline comparison
│   └── config.yaml            # Config used
├── history.json               # Historical results
└── latest -> 20260102_143022  # Symlink to latest
```

### Executive Summary Template

```python
def generate_executive_summary(result: BenchmarkResult) -> str:
    """Generate 1-page executive summary."""

    # Determine pass/fail
    thresholds = {
        "faithfulness": 0.80,
        "context_recall": 0.80,
        "aggregate_score": 0.75,
    }

    status_emoji = "✅" if all(
        result.harness_result.aggregates[k].mean >= v
        for k, v in thresholds.items()
        if k in result.harness_result.aggregates
    ) else "❌"

    return f"""# Retrieval Benchmark Results {status_emoji}

**Date:** {result.timestamp.strftime("%Y-%m-%d %H:%M")}
**Duration:** {result.duration_seconds/60:.1f} minutes
**Runs:** {result.harness_result.num_runs}

## Key Metrics

| Metric | Score | Threshold | Status |
|--------|-------|-----------|--------|
| Faithfulness | {result.harness_result.aggregates["faithfulness"].mean:.3f} | ≥0.80 | {"✅" if result.harness_result.aggregates["faithfulness"].mean >= 0.80 else "❌"} |
| Context Recall | {result.harness_result.aggregates["context_recall"].mean:.3f} | ≥0.80 | {"✅" if result.harness_result.aggregates["context_recall"].mean >= 0.80 else "❌"} |
| Aggregate | {result.harness_result.aggregates["aggregate_score"].mean:.3f} | ≥0.75 | {"✅" if result.harness_result.aggregates["aggregate_score"].mean >= 0.75 else "❌"} |

## Baseline Comparison

{generate_baseline_comparison(result)}

## Summary

{generate_summary_paragraph(result)}

---
Generated by draagon-ai benchmark suite v{VERSION}
"""
```

### Detailed Report Template

```python
def generate_detailed_report(result: BenchmarkResult) -> str:
    """Generate comprehensive report with all details."""

    return f"""# Detailed Benchmark Report

## Configuration

```yaml
{yaml.dump(result.config.__dict__, default_flow_style=False)}
```

## Overall Results

{generate_summary_table(result)}

## Statistical Details

{generate_statistical_details(result)}

## Category Breakdown

{generate_category_breakdown(result)}

## Difficulty Breakdown

{generate_difficulty_breakdown(result)}

## Query Type Breakdown

{generate_query_type_breakdown(result)}

## High-Variance Queries

{generate_high_variance_section(result)}

## Baseline Comparison

{generate_detailed_comparison(result)}

## Run Details

{generate_run_details(result)}

## Methodology

- **Corpus:** {len(result.corpus)} documents across {len(set(d.category for d in result.corpus.documents))} categories
- **Queries:** {len(result.queries.queries)} queries
- **Embedding Model:** mxbai-embed-large (1024-dim)
- **LLM Evaluator:** Groq Llama 3.3 70B
- **Runs:** {result.harness_result.num_runs} with seeds {result.config.base_seed} to {result.config.base_seed + result.harness_result.num_runs - 1}

---
Generated: {datetime.now().isoformat()}
"""
```

### Per-Query CSV

```python
def generate_per_query_csv(result: BenchmarkResult) -> str:
    """Generate CSV with per-query results."""
    import io
    import csv

    output = io.StringIO()
    writer = csv.writer(output)

    # Header
    writer.writerow([
        "query_id", "query_type", "difficulty", "category",
        "faithfulness_mean", "faithfulness_std",
        "recall_mean", "recall_std",
        "precision_mean", "precision_std",
        "relevancy_mean", "relevancy_std",
        "is_high_variance",
    ])

    # Per-query rows
    for query_id, query_results in result.per_query_results.items():
        query = result.queries.get(query_id)
        writer.writerow([
            query_id,
            query.query_type.value,
            query.difficulty.value,
            query.target_category.value,
            query_results["faithfulness"]["mean"],
            query_results["faithfulness"]["std"],
            query_results["recall"]["mean"],
            query_results["recall"]["std"],
            query_results["precision"]["mean"],
            query_results["precision"]["std"],
            query_results["relevancy"]["mean"],
            query_results["relevancy"]["std"],
            query_results["is_high_variance"],
        ])

    return output.getvalue()
```

### Historical Tracking

```python
@dataclass
class HistoricalEntry:
    timestamp: datetime
    run_id: str
    metrics: dict[str, float]
    config_hash: str


def append_to_history(result: BenchmarkResult, history_path: Path):
    """Append run to history file."""
    history = []

    if history_path.exists():
        with open(history_path) as f:
            history = json.load(f)

    entry = {
        "timestamp": result.timestamp.isoformat(),
        "run_id": result.timestamp.strftime("%Y%m%d_%H%M%S"),
        "metrics": {
            name: agg.mean
            for name, agg in result.harness_result.aggregates.items()
        },
        "duration_seconds": result.duration_seconds,
    }

    history.append(entry)

    # Keep last 100 runs
    history = history[-100:]

    with open(history_path, "w") as f:
        json.dump(history, f, indent=2)


def detect_regression(
    current: BenchmarkResult,
    history: list[dict],
    threshold: float = 0.05,
) -> list[str]:
    """Detect if current run regressed vs recent history."""
    if len(history) < 2:
        return []

    regressions = []
    recent_avg = {}

    # Calculate average of last 5 runs
    for metric in ["faithfulness", "context_recall", "aggregate_score"]:
        values = [h["metrics"].get(metric, 0) for h in history[-5:]]
        recent_avg[metric] = sum(values) / len(values) if values else 0

    # Check for regression
    for metric, avg in recent_avg.items():
        current_val = current.harness_result.aggregates.get(metric, 0)
        if isinstance(current_val, AggregateResult):
            current_val = current_val.mean

        if avg > 0 and (avg - current_val) / avg > threshold:
            regressions.append(
                f"{metric}: {current_val:.3f} vs avg {avg:.3f} (-{(avg-current_val)/avg*100:.1f}%)"
            )

    return regressions
```

### Report Writer

```python
class ReportWriter:
    def __init__(self, output_dir: Path):
        self.output_dir = output_dir
        self.run_dir = output_dir / datetime.now().strftime("%Y%m%d_%H%M%S")
        self.run_dir.mkdir(parents=True, exist_ok=True)

        # Update latest symlink
        latest = output_dir / "latest"
        if latest.is_symlink():
            latest.unlink()
        latest.symlink_to(self.run_dir.name)

    def write_all(self, result: BenchmarkResult):
        """Write all report formats."""
        # Executive summary
        (self.run_dir / "summary.md").write_text(
            generate_executive_summary(result)
        )

        # Full report
        (self.run_dir / "report.md").write_text(
            generate_detailed_report(result)
        )

        # JSON results
        (self.run_dir / "results.json").write_text(
            json.dumps(result.to_dict(), indent=2)
        )

        # Per-query CSV
        (self.run_dir / "per_query.csv").write_text(
            generate_per_query_csv(result)
        )

        # Comparison report
        if result.baseline_results:
            (self.run_dir / "comparison.md").write_text(
                generate_comparison_report(result)
            )

        # Config
        (self.run_dir / "config.yaml").write_text(
            yaml.dump(result.config.__dict__, default_flow_style=False)
        )

        # Update history
        append_to_history(result, self.output_dir / "history.json")

        # Check for regressions
        history = json.loads((self.output_dir / "history.json").read_text())
        regressions = detect_regression(result, history[:-1])
        if regressions:
            logger.warning(f"⚠️ Regressions detected: {regressions}")
            (self.run_dir / "regressions.txt").write_text("\n".join(regressions))

        logger.info(f"Reports written to {self.run_dir}")
```

---

## Testing Requirements

### Unit Tests
```python
def test_executive_summary_generation():
    """Executive summary has required sections."""
    result = BenchmarkResult(...)
    summary = generate_executive_summary(result)

    assert "# Retrieval Benchmark Results" in summary
    assert "Key Metrics" in summary
    assert "Baseline Comparison" in summary

def test_csv_format():
    """Per-query CSV is valid."""
    result = BenchmarkResult(...)
    csv_data = generate_per_query_csv(result)

    reader = csv.DictReader(io.StringIO(csv_data))
    rows = list(reader)

    assert len(rows) == len(result.queries.queries)
    assert "query_id" in rows[0]
    assert "faithfulness_mean" in rows[0]

def test_regression_detection():
    """Regression detected when metric drops."""
    history = [
        {"metrics": {"faithfulness": 0.85}},
        {"metrics": {"faithfulness": 0.86}},
        {"metrics": {"faithfulness": 0.84}},
    ]

    result = BenchmarkResult(...)
    result.harness_result.aggregates["faithfulness"] = AggregateResult(mean=0.75, ...)

    regressions = detect_regression(result, history, threshold=0.05)
    assert len(regressions) > 0
    assert "faithfulness" in regressions[0]
```

---

## Files to Create/Modify

- `src/draagon_ai/testing/benchmarks/reports/__init__.py`
- `src/draagon_ai/testing/benchmarks/reports/executive.py`
- `src/draagon_ai/testing/benchmarks/reports/detailed.py`
- `src/draagon_ai/testing/benchmarks/reports/csv_export.py`
- `src/draagon_ai/testing/benchmarks/reports/history.py`
- `src/draagon_ai/testing/benchmarks/reports/writer.py`
- Add tests to `tests/benchmarks/test_reports.py`

---

## Definition of Done

- [ ] Executive summary with pass/fail status
- [ ] Detailed report with all breakdowns
- [ ] Per-query CSV export
- [ ] Baseline comparison report
- [ ] Historical tracking
- [ ] Regression detection
- [ ] Latest symlink maintained
- [ ] All templates render correctly
- [ ] Tests passing
